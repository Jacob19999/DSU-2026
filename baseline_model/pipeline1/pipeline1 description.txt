PIPELINE 1: DIRECT VOLUME PREDICTION - COMPREHENSIVE PLAN
==========================================================

OVERVIEW
--------
Pipeline 1 is a direct volume prediction approach that uses machine learning models 
(ensemble methods, XGBoost, etc.) to forecast ED volumes directly from the entire dataset.
The pipeline forecasts total encounters (ED Enc) and admitted encounters (ED Enc Admitted)
for each Site, Date, and 6-hour time Block combination.

TARGET FORECAST PERIOD
----------------------
- September 1-30, 2025
- October 1-31, 2025
- 4 Sites (A, B, C, D)
- 4 Time Blocks per day (0-5, 6-11, 12-17, 18-23)
- 2 Metrics: Total encounters, Admitted encounters


STEP 1: DATA PREPROCESSING
---------------------------

1.1 Data Loading
    - Load DSU-Dataset.csv from Dataset/ directory
    - Convert Date column to datetime format
    - Validate data integrity (check for missing values, data types)
    - Log basic statistics (shape, date range, site distribution)

1.2 Data Cleaning
    - Handle missing values:
      * Missing dates: drop or interpolate based on context
      * Missing hours: validate against 0-23 range
      * Missing Site: drop invalid records
      * Missing ED Enc/ED Enc Admitted: set to 0 or drop based on business logic
    - Validate data consistency:
      * Ensure ED Enc Admitted <= ED Enc for all records
      * Check for negative values in count columns
      * Validate Site values are in {A, B, C, D}
      * Validate Hour values are in {0-23}

1.3 Data Aggregation
    - Aggregate hourly data to 6-hour time blocks:
      * Block 0: Hours 0-5 (midnight to 5:59 AM)
      * Block 1: Hours 6-11 (6:00 AM to 11:59 AM)
      * Block 2: Hours 12-17 (noon to 5:59 PM)
      * Block 3: Hours 18-23 (6:00 PM to 11:59 PM)
    - Group by: Site, Date, Block
    - Aggregate: Sum of ED Enc, Sum of ED Enc Admitted
    - Calculate derived metrics:
      * Admission rate = ED Enc Admitted / ED Enc (with zero-division handling)

1.4 Data Splitting
    - Create validation splits using time-based cross-validation:
      * Period 1: Train up to 2024-12-31, Test 2025-01-01 to 2025-02-28
      * Period 2: Train up to 2025-02-28, Test 2025-03-01 to 2025-04-30
      * Period 3: Train up to 2025-04-30, Test 2025-05-01 to 2025-06-30
      * Period 4: Train up to 2025-06-30, Test 2025-07-01 to 2025-08-31
    - Final training set: All data up to 2025-08-31
    - Test set: September-October 2025 (to be forecasted)


STEP 2: FEATURE ENGINEERING
----------------------------

2.1 Temporal Features
    - Calendar features:
      * Day of week (0-6, Monday=0)
      * Month (1-12)
      * Day of month (1-31)
      * Day of year (1-365/366)
      * Week of year (1-52/53)
      * Quarter (1-4)
      * Year
      * Is weekend (boolean)
      * Is month start/end (boolean)
      * Is quarter start/end (boolean)
    - Cyclical encoding:
      * Day of week (sin/cos encoding)
      * Month (sin/cos encoding)
      * Day of year (sin/cos encoding)
      * Hour block (sin/cos encoding)

2.2 Lag Features
    - Create lagged values grouped by (Site, Block):
      * Lag 1: Previous day's volume
      * Lag 7: Same day of week, previous week
      * Lag 14: Same day of week, 2 weeks ago
      * Lag 28: Same day of week, 4 weeks ago
      * Lag 30: Previous month, same day
      * Lag 90: Previous quarter, same day
      * Lag 365: Previous year, same day (if available)
    - Apply to both: total_enc, admitted_enc, admit_rate

2.3 Rolling Window Features
    - Rolling statistics grouped by (Site, Block):
      * Rolling mean: 7-day, 14-day, 28-day, 90-day windows
      * Rolling median: 7-day, 14-day, 28-day windows
      * Rolling std: 7-day, 14-day, 28-day windows
      * Rolling min/max: 7-day, 14-day windows
    - Apply to: total_enc, admitted_enc, admit_rate

2.4 Site-Specific Features
    - Site identifier (one-hot encoded or label encoded)
    - Site-specific historical averages:
      * Mean daily volume per site
      * Mean admission rate per site
      * Site-specific seasonal patterns

2.5 Block-Specific Features
    - Block identifier (0, 1, 2, 3)
    - Block-specific historical averages:
      * Mean volume per block
      * Block-specific patterns (e.g., morning vs evening)

2.6 Interaction Features
    - Site × Block interactions
    - Site × Day of week interactions
    - Block × Day of week interactions
    - Site × Month interactions

2.7 External Covariates (Optional but Recommended)
    - Weather data (if available):
      * Temperature, precipitation, extreme weather events
    - Holiday indicators:
      * US federal holidays
      * Regional/local holidays
    - Special events:
      * Known events that affect ED volume (sports events, festivals, etc.)

2.8 Target-Derived Features
    - Historical admission rate (rolling average)
    - Volume trends (slope over recent periods)
    - Volatility measures (coefficient of variation)

2.9 Feature Selection
    - Remove constant features (zero variance)
    - Remove highly correlated features (correlation > 0.95)
    - Handle missing values in engineered features:
      * Forward fill for lag features
      * Mean imputation for rolling features
      * Drop rows with critical missing values if necessary


STEP 3: TRAINING
----------------

3.1 Model Selection
    - Primary model: XGBoost Regressor
      * Advantages: Handles non-linear relationships, feature importance, robust to outliers
    - Alternative models to consider:
      * LightGBM (faster training, similar performance)
      * Random Forest (baseline ensemble)
      * Linear models (Ridge/Lasso) for interpretability
      * Neural networks (if sufficient data)

3.2 Model Architecture
    - Two-model approach:
      * Model 1: Predict total_enc (total encounters)
      * Model 2: Predict admit_rate (admission rate)
      * Derived: admitted_enc = total_enc × admit_rate (with constraints)
    - Alternative: Single model for admitted_enc directly
    - Objective functions:
      * Total encounters: Poisson regression or squared error
      * Admission rate: Squared error or logistic regression
      * Admitted encounters: Constrained prediction (min(total_enc × rate, total_enc))

3.3 Training Configuration
    - Hyperparameters (initial):
      * n_estimators: 100-500
      * max_depth: 3-8
      * learning_rate: 0.01-0.1
      * subsample: 0.8-1.0
      * colsample_bytree: 0.8-1.0
      * min_child_weight: 1-5
      * reg_alpha: 0-1 (L1 regularization)
      * reg_lambda: 0-1 (L2 regularization)
    - Early stopping:
      * Use validation set for early stopping
      * Monitor metric: WAPE (Weighted Absolute Percentage Error) or MAE
      * Patience: 20-50 rounds

3.4 Training Process
    - For each validation period:
      * Split data into train/validation sets
      * Train model on training set
      * Evaluate on validation set
      * Store model and metrics
    - Final model training:
      * Train on all available historical data (up to 2025-08-31)
      * Use best hyperparameters from validation
      * Save final model artifacts

3.5 Handling Data Leakage
    - Ensure no future information leaks into features
    - Lag features must use only past data
    - Rolling features must use only past data
    - External covariates must be available at prediction time
    - Validate feature creation logic for temporal correctness


STEP 4: TUNING
--------------

4.1 Hyperparameter Tuning Strategy
    - Method: Bayesian Optimization (Optuna) or Grid/Random Search
    - Search space: Define ranges for all hyperparameters
    - Objective: Minimize validation WAPE or MAE
    - Cross-validation: Use 4 validation periods for robust evaluation

4.2 Tuning Process
    - For each validation period:
      * Perform hyperparameter search
      * Evaluate on validation set
      * Track best parameters per period
    - Aggregate results:
      * Average performance across all validation periods
      * Select hyperparameters that perform best on average
      * Consider stability (low variance across periods)

4.3 Feature Engineering Tuning
    - Experiment with different lag windows
    - Test different rolling window sizes
    - Evaluate feature importance from trained models
    - Remove low-importance features
    - Test interaction features

4.4 Model Ensemble Tuning
    - If using multiple models:
      * Test different ensemble methods (weighted average, stacking)
      * Optimize ensemble weights
      * Evaluate ensemble performance vs single model

4.5 Regularization Tuning
    - Adjust L1/L2 regularization to prevent overfitting
    - Monitor training vs validation performance
    - Ensure model generalizes across different time periods


STEP 5: VALIDATION
------------------

5.1 Validation Strategy
    - Time-based cross-validation (4 periods)
    - No random shuffling (preserve temporal order)
    - Each period uses 2-month test windows
    - Final evaluation: Average metrics across all 4 periods

5.2 Validation Metrics
    - Primary metrics:
      * WAPE (Weighted Absolute Percentage Error) - competition metric
      * MAE (Mean Absolute Error)
      * RMSE (Root Mean Squared Error)
    - Secondary metrics:
      * MAPE (Mean Absolute Percentage Error)
      * R² (Coefficient of determination)
      * Median Absolute Error
    - Site-specific metrics:
      * WAPE per site
      * MAE per site
    - Block-specific metrics:
      * WAPE per block
      * MAE per block

5.3 Validation Process
    - For each validation period (1-4):
      * Train model on training data
      * Predict on validation period
      * Calculate all metrics
      * Store predictions and metrics
    - Aggregate results:
      * Average WAPE across all periods
      * Average MAE across all periods
      * Site-level performance breakdown
      * Block-level performance breakdown
      * Time-series plots of predictions vs actuals

5.4 Error Analysis
    - Identify patterns in errors:
      * Which sites have highest errors?
      * Which blocks have highest errors?
      * Which time periods have highest errors?
      * Are errors systematic (bias) or random (variance)?
    - Analyze residuals:
      * Residual plots (predicted vs actual)
      * Residual distribution
      * Temporal patterns in residuals

5.5 Model Diagnostics
    - Feature importance analysis:
      * Top 20 most important features
      * Site-specific feature importance
      * Block-specific feature importance
    - Prediction quality checks:
      * Ensure no negative predictions
      * Ensure admitted_enc <= total_enc
      * Check for unrealistic spikes or drops
    - Stability checks:
      * Performance consistency across validation periods
      * Feature importance consistency

5.6 Final Model Selection
    - Compare models across validation periods
    - Select model with best average WAPE
    - Ensure model is stable (low variance across periods)
    - Document final model configuration and performance


STEP 6: PREDICTION GENERATION
------------------------------

6.1 Final Model Training
    - Train final model on all data up to 2025-08-31
    - Use best hyperparameters from validation
    - Save model artifacts for production use

6.2 Forecast Generation
    - Generate features for September-October 2025:
      * Calendar features (known)
      * Lag features (from historical data)
      * Rolling features (from historical data)
      * External covariates (if available)
    - Generate predictions:
      * Predict total_enc for each (Site, Date, Block)
      * Predict admit_rate for each (Site, Date, Block)
      * Calculate admitted_enc = total_enc × admit_rate
      * Apply constraints (admitted_enc <= total_enc, no negatives)

6.3 Output Format
    - Create output DataFrame with columns:
      * Site
      * Date
      * Block
      * Predicted_Total_Enc
      * Predicted_Admitted_Enc
    - Save to CSV/Parquet format
    - Validate output format matches competition requirements

6.4 Post-Processing
    - Apply business rules:
      * Round predictions to integers (if required)
      * Ensure non-negative values
      * Ensure admitted <= total
    - Quality checks:
      * Compare predictions to historical patterns
      * Flag unusual predictions for review
      * Validate against known constraints


STEP 7: DOCUMENTATION AND ARTIFACTS
------------------------------------

7.1 Model Artifacts
    - Save trained models (pickle/joblib format)
    - Save feature engineering pipeline
    - Save preprocessing steps
    - Save hyperparameters

7.2 Documentation
    - Document feature engineering logic
    - Document model architecture
    - Document hyperparameters and tuning process
    - Document validation results
    - Document assumptions and limitations

7.3 Reproducibility
    - Save random seeds
    - Document data versions
    - Document code versions
    - Create requirements.txt with exact package versions

7.4 Results Summary
    - Validation metrics summary table
    - Feature importance plots
    - Prediction vs actual plots
    - Error analysis report


IMPLEMENTATION NOTES
--------------------

- Use existing data_ingestion/loader.py utilities for data loading and splitting
- Leverage existing feature engineering modules in src/dsu_forecast/features/
- Use existing modeling utilities in src/dsu_forecast/modeling/
- Follow existing code structure and patterns
- Ensure code is modular and testable
- Add logging for debugging and monitoring
- Handle edge cases (missing data, zero volumes, etc.)
- Optimize for performance (use vectorized operations, caching)
